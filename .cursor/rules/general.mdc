---
description: Understanding the project
globs: 
alwaysApply: true
---
This repository's `workflow` folder contains scripts and documentation for a workflow in which a Claude 3.7-powered Cursor Agent:

1. Scraped code examples from https://htmx.org/examples
2. Used the `llm` CLI tool to transform the scraped examples into structured JSON data
3. Created a serverless PostgreSQL database on Digital Ocean with the help of the `doctl` CLI tool and set up users and secrets for DB management
4. Uploaded the structured examples data to the PostgreSQL database
5. Embedded the examples with a Google text embeddings model and uploaded the embeddings to the PostgreSQL database
6. Created a PL/pgSQL function to power semantic similarity search
7. Created and deployed a public-facing PostgREST API on a Digital Ocean droplet that uses the `web_anon` role to run similarity search over the embedded HTMX examples

The workflow has been created and documented with an eye to repeatability by future AI agents with minimal human intervention. The steps mostly involve MCP tool calls, CLI commands, and bash and python scripts, not web interfaces.

Remember to *never* hard-code secrets in your scripts. Secrets may be written to `.env` and programmatically retrieved from there. Feel free to request human intervention for generating and managing secrets; you should never be using placeholder secrets.

Working documents and helper files that won't ultimately be retained may be placed in the `artifacts` folder.

To avoid confusion, you should keep your bash console opened to the project root folder and should specify all file paths in relation to the project root.
